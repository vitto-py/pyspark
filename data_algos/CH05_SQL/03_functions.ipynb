{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-In Functions\n",
    "These built-in functions are designed to generate optimized code for execution at\n",
    "runtime \n",
    "> they are designed to take one\n",
    "or more columns of the same row as the input, and they return only a single column as\n",
    "the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Category       | Description                                                                                                                 |\n",
    "|----------------|-----------------------------------------------------------------------------------------------------------------------------|\n",
    "| Date time      | unix_timestamp, from_unixtime, to_date, current_date, current_timestamp, date_add, date_sub, add_months, datediff, months_between, dayofmonth, dayofyear, weekofyear, second, minute, hour, month, make_date, make_timestamp, make_interval |\n",
    "| String         | concat, length, levenshtein, locate, lower, upper, ltrim, rtrim, trim, lpad, rpad, repeat, reverse, split, substring, base64 |\n",
    "| Math           | cos, acos, sin, asin, tan, atan, ceil, floor, exp, factorial, log, pow, radian, degree, sqrt, hex, unhex                    |\n",
    "| Cryptography   | crc32, hash, md5, sha1, sha2                                                                                                 |\n",
    "| Aggregation    | approx_count_distinct, countDistinct, sumDistinct, avg, corr, count, first, last, max, min, skewness, sum                    |\n",
    "| Collection     | array_contains, explode, from_json, size, sort_array, to_json                                                                |\n",
    "| Window         | dense_rank, lag, lead, ntile, rank, row_number                                                                               |\n",
    "| Misc.          | coalesce, isNan, isnull, isNotNull, monotonically_increasing_id, lit, when                                                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 14:12:56 WARN Utils: Your hostname, msi-MAG resolves to a loopback address: 127.0.1.1; using 192.168.0.129 instead (on interface wlp3s0)\n",
      "24/05/24 14:12:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/24 14:12:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+----------+-------------------+\n",
      "|id |date      |timestamp              |date_str  |ts_str             |\n",
      "+---+----------+-----------------------+----------+-------------------+\n",
      "|1  |2018-01-01|2018-01-01 15:04:58:865|01-01-2018|12-05-2017 00:45:50|\n",
      "+---+----------+-----------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the last two columns don't follow the default date format\n",
    "testDF = spark.createDataFrame([(1, \"2018-01-01\", \"2018-01-01 15:04:58:865\", \n",
    "  \"01-01-2018\", \"12-05-2017 00:45:50\")], [\"id\", \"date\", \"timestamp\", \"date_str\", \"ts_str\"])\n",
    "\n",
    "testDF.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert strings to dates and timestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testResultDF = testDF.select(\n",
    "    F.to_date(\"date\").alias(\"date1\"),\n",
    "    F.to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss:SSS\").alias(\"ts1\"),  # Specify format with milliseconds\n",
    "    F.to_date(\"date_str\", \"MM-dd-yyyy\").alias(\"date2\"),\n",
    "    F.to_timestamp(\"ts_str\", \"MM-dd-yyyy HH:mm:ss\").alias(\"ts2\"),  # Corrected format with padding\n",
    "    F.unix_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss:SSS\").alias(\"unix_ts\")  # Specify format for unix_timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date1: date (nullable = true)\n",
      " |-- ts1: timestamp (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- ts2: timestamp (nullable = true)\n",
      " |-- unix_ts: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----------+-----------------------+----------+-------------------+----------+\n",
      "|date1     |ts1                    |date2     |ts2                |unix_ts   |\n",
      "+----------+-----------------------+----------+-------------------+----------+\n",
      "|2018-01-01|2018-01-01 15:04:58.865|2018-01-01|2017-12-05 00:45:50|1514815498|\n",
      "+----------+-----------------------+----------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(testResultDF.printSchema())\n",
    "\n",
    "testResultDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get internal of a date\n",
    "> * get day\n",
    "> * get month\n",
    "> * get year\n",
    "> * get hour\n",
    "> * get minute \n",
    "> * get second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----+------+------+----+-----+---+---+------------+-----------+\n",
      "|id |input                  |hour|minute|second|year|month|day|doy|week_of_year|day_of_week|\n",
      "+---+-----------------------+----+------+------+----+-----+---+---+------------+-----------+\n",
      "|1  |2020-02-01 11:01:19.06 |11  |1     |19    |2020|2    |1  |32 |5           |7          |\n",
      "|2  |2019-03-01 12:01:19.406|12  |1     |19    |2019|3    |1  |60 |9           |6          |\n",
      "|3  |2021-03-01 12:01:19.406|12  |1     |19    |2021|3    |1  |60 |9           |2          |\n",
      "+---+-----------------------+----+------+------+----+-----+---+---+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hour, minute,second\n",
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "\n",
    "df3.select(\n",
    "    F.col(\"id\"),\n",
    "    F.col(\"input\"), \n",
    "    F.hour(F.col(\"input\")).alias(\"hour\"), \n",
    "    F.minute(F.col(\"input\")).alias(\"minute\"),\n",
    "    F.second(F.col(\"input\")).alias(\"second\"),\n",
    "    F.year(F.col(\"input\")).alias(\"year\"), \n",
    "    F.month(F.col(\"input\")).alias(\"month\"),\n",
    "    F.dayofmonth(F.col(\"input\")).alias(\"day\"),\n",
    "    F.dayofyear(F.col(\"input\")).alias(\"doy\"),\n",
    "    F.weekofyear(F.col(\"input\")).alias(\"week_of_year\"),\n",
    "    F.dayofweek(F.col(\"input\")).alias(\"day_of_week\")\n",
    "  ).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Operation between 2 dates**\n",
    "> * datediff\n",
    "> * months_between\n",
    "> * last_day of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+\n",
      "|name| join_date|leave_date|\n",
      "+----+----------+----------+\n",
      "|John|2016-01-01|2017-10-15|\n",
      "| May|2017-02-06|2017-12-25|\n",
      "+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeData = spark.createDataFrame([(\"John\", \"2016-01-01\", \"2017-10-15\"),\n",
    "(\"May\", \"2017-02-06\", \"2017-12-25\")], (\"name\", \"join_date\", \"leave_date\"))\n",
    "employeeData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+---------------+\n",
      "|name|n_days|   n_months|last_day_of_mon|\n",
      "+----+------+-----------+---------------+\n",
      "|John|   653| 21.4516129|     2017-10-31|\n",
      "| May|   322|10.61290323|     2017-12-31|\n",
      "+----+------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeData.select(\"name\",\n",
    "            F.datediff(\"leave_date\", \"join_date\").alias(\"n_days\"),\n",
    "            F.months_between(\"leave_date\", \"join_date\").alias(\"n_months\"),\n",
    "            F.last_day(\"leave_date\").alias(\"last_day_of_mon\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Aggregations over a Date**\n",
    "* add days\n",
    "* sub days\n",
    "* add months\n",
    "* sub months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  new_year|\n",
      "+----------+\n",
      "|2018-01-01|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# perform date addition and subtraction\n",
    "oneDate = spark.createDataFrame([(\"2018-01-01\",)], [\"new_year\"])\n",
    "oneDate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+------------------+-------------------+\n",
      "|  new_year|date_plus_10_days|date_minus_10_days|date_plus_2_months|date_minus_2_months|\n",
      "+----------+-----------------+------------------+------------------+-------------------+\n",
      "|2018-01-01|       2018-01-11|        2017-12-22|        2018-03-01|         2017-11-01|\n",
      "+----------+-----------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oneDate.select(F.col(\"new_year\"),\n",
    "    F.date_add(\"new_year\", 10).alias(\"date_plus_10_days\"),\n",
    "    F.date_sub(\"new_year\", 10).alias(\"date_minus_10_days\"),\n",
    "    F.add_months(\"new_year\", 2).alias(\"date_plus_2_months\"),\n",
    "    F.add_months(\"new_year\", -2).alias(\"date_minus_2_months\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to current date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+\n",
      "|new_year  |current_date|datediff|\n",
      "+----------+------------+--------+\n",
      "|2018-01-01|2024-05-20  |-2331   |\n",
      "+----------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# current_timestamp()\n",
    "oneDate.select(F.col(\"new_year\"),F.current_date().alias(\"current_date\"),\n",
    "               F.datediff(oneDate.new_year, F.current_date()).alias(\"datediff\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trunc(column, format)\t\n",
    "Truncate a date or timestamp column in a DataFrame to a specified level of granularity. For example, `trunc(df['date_column'], 'month')` would truncate the dates in the `“date_column”` of DataFrame `“df”` to the first day of the month, effectively removing the day component and retaining only the month and year.\n",
    "> Possible levels\n",
    "> * Month\n",
    "> * Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------+----------+\n",
      "|input                  |Keep_Month|Keep_Year |\n",
      "+-----------------------+----------+----------+\n",
      "|2020-02-01 11:01:19.06 |2020-02-01|2020-01-01|\n",
      "|2019-03-01 12:01:19.406|2019-03-01|2019-01-01|\n",
      "|2021-03-01 12:01:19.406|2021-03-01|2021-01-01|\n",
      "+-----------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(F.col(\"input\"), \n",
    "    F.trunc(F.col(\"input\"),\"Month\").alias(\"Keep_Month\"), \n",
    "    F.trunc(F.col(\"input\"),\"Year\").alias(\"Keep_Year\") \n",
    "   ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advance: Bucketing\n",
    "### Date and Timestamp Window Functions\n",
    "| DATE & TIME WINDOW FUNCTION SYNTAX | DATE & TIME WINDOW FUNCTION DESCRIPTION |\n",
    "|------------------------------------|-----------------------------------------|\n",
    "| window(timeColumn, windowDuration, slideDuration, startTime) | Bucketize rows into one or more time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, [12:05,12:10). Windows can support microsecond precision. Windows in the order of months are not supported. |\n",
    "| window(timeColumn, windowDuration, slideDuration) | Bucketize rows into one or more time windows given a timestamp specifying column. e.g. [12:05,12:10). Windows can support microsecond precision. Windows in the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC |\n",
    "| window(timeColumn, windowDuration) | Generates tumbling time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, e.g. [12:05,12:10) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strings\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|   Spark |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF = spark.createDataFrame([(\"   Spark \",)], [\"name\"])\n",
    "sparkDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+\n",
      "| trim| ltrim|   rtrim|\n",
      "+-----+------+--------+\n",
      "|Spark|Spark |   Spark|\n",
      "+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trim removes spaces on both sides of a string\n",
    "# ltrim only removes spaces on the left side of a string\n",
    "# rtrim only removes spaces on the right side of a string\n",
    "sparkDF.select(\n",
    "    F.trim(\"name\").alias(\"trim\"),\n",
    "    F.ltrim(\"name\").alias(\"ltrim\"),\n",
    "    F.rtrim(\"name\").alias(\"rtrim\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding** <br>\n",
    "Padding = add values to complete a given lenght <br>\n",
    "first trim spaces around string \"Spark\" and then pad it so the final\n",
    "length is 8 characters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|        lpad|        rpad|\n",
      "+------------+------------+\n",
      "|-------Spark|Spark=======|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lpad pads the left side of the trim column with - to the length of 8\n",
    "# rpad pads the right side of the trim colum with = to the length of 8\n",
    "sparkDF.select(F.trim(\"name\").alias(\"trim\")).select(\n",
    "    F.lpad(\"trim\", 12, \"-\").alias(\"lpad\"),\n",
    "    F.rpad(\"trim\", 12, \"=\").alias(\"rpad\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+\n",
      "|subject|verb|    adj|\n",
      "+-------+----+-------+\n",
      "|  Spark|  is|awesome|\n",
      "+-------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkAwesomeDF = spark.createDataFrame([(\"Spark\", \"is\", \"awesome\")], [\"subject\", \"verb\", \"adj\"])\n",
    "sparkAwesomeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+----------------+\n",
      "|           lower|           upper|         initcap|         reverse|\n",
      "+----------------+----------------+----------------+----------------+\n",
      "|spark is awesome|SPARK IS AWESOME|Spark Is Awesome|emosewa si krapS|\n",
      "+----------------+----------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.concat_ws(\"_separator_\", \"part1\", \"part2\", ...) = part1_separator_part2_separator_part3\n",
    "sparkAwesomeDF.select(F.concat_ws(\" \",\"subject\", \"verb\",\n",
    "\"adj\").alias(\"sentence\")).select(\n",
    "    F.lower(\"sentence\").alias(\"lower\"),\n",
    "    F.upper(\"sentence\").alias(\"upper\"),\n",
    "    F.initcap(\"sentence\").alias(\"initcap\"),\n",
    "    F.reverse(\"sentence\").alias(\"reverse\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace = translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|subject|translate|\n",
      "+-------+---------+\n",
      "|  Spark|    Spock|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# translate from one character to another\n",
    "sparkAwesomeDF.select(\"subject\", F.translate(F.col(\"subject\"), \"ar\",\"oc\").alias(\"translate\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|rhyme                                                      |\n",
      "+-----------------------------------------------------------+\n",
      "|A fox saw a crow sitting on a tree singing \"Caw! Caw! Caw!\"|\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rhymeDF = spark.createDataFrame([(\"A fox saw a crow sitting on a tree singing \\\"Caw! Caw! Caw!\\\"\", )],[\"rhyme\"])\n",
    "rhymeDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|substring|\n",
      "+---------+\n",
      "|      fox|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rhymeDF.select(F.regexp_extract(\"rhyme\",\"[a-z]*o*[xw]\",0)\n",
    ".alias(\"substring\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`regexp_extract_all` is available from Spark 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|new_rhyme                                                       |\n",
      "+----------------------------------------------------------------+\n",
      "|A animal saw a animal sitting on a tree singing \"Caw! Caw! Caw!\"|\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rhymeDF.select(F.regexp_replace(\"rhyme\", \"fox|crow\", \"animal\").alias(\"new_rhyme\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Math Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+\n",
      "|    pie|gpa|year|\n",
      "+-------+---+----+\n",
      "|3.14159|3.5|2018|\n",
      "+-------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numberDF = spark.createDataFrame([(3.14159, 3.5, 2018),], [\"pie\",\"gpa\", \"year\"])\n",
    "numberDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+----+\n",
      "|pie0|pie1|pie2|gpa|year|\n",
      "+----+----+----+---+----+\n",
      "| 3.0| 3.1|3.14|4.0|2018|\n",
      "+----+----+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numberDF.select(\n",
    "    F.round(\"pie\").alias(\"pie0\"),\n",
    "    F.round(\"pie\", 1).alias(\"pie1\"),\n",
    "    F.round(\"pie\", 2).alias(\"pie2\"),\n",
    "    F.round(\"gpa\").alias(\"gpa\"),\n",
    "    F.round(\"year\").alias(\"year\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Collection Functions\n",
    "The collection functions are designed to work with complex data types such as arrays,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- day: string (nullable = true)\n",
      " |-- tasks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n",
      "+------+----------------------------------+\n",
      "|day   |tasks                             |\n",
      "+------+----------------------------------+\n",
      "|Monday|[Pick Up John, Buy Milk, Pay Bill]|\n",
      "+------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasksDF = spark.createDataFrame( [(\"Monday\", [\"Pick Up John\",\n",
    "\"Buy Milk\", \"Pay Bill\"]), ], [\"day\", \"tasks\"])\n",
    "\n",
    "# schema of tasksDF\n",
    "print(tasksDF.printSchema())\n",
    "tasksDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------------------------------+-------+\n",
      "|day   |size|sorted_tasks                      |payBill|\n",
      "+------+----+----------------------------------+-------+\n",
      "|Monday|3   |[Buy Milk, Pay Bill, Pick Up John]|true   |\n",
      "+------+----+----------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasksDF.select(\"day\", \n",
    "                F.size(F.col(\"tasks\")).alias(\"size\"),\n",
    "                F.sort_array(F.col(\"tasks\")).alias(\"sorted_tasks\"),\n",
    "                F.array_contains(F.col(\"tasks\"), \"Pay Bill\").alias(\"payBill\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// the explode function will create a new row for each element in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|   day|         col|\n",
      "+------+------------+\n",
      "|Monday|Pick Up John|\n",
      "|Monday|    Buy Milk|\n",
      "|Monday|    Pay Bill|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasksDF.select(\"day\", F.explode(\"tasks\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> from JSON to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- todos_str: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-----------------------------------------------------------------+\n",
      "|todos_str                                                        |\n",
      "+-----------------------------------------------------------------+\n",
      "|{\"day\": \"Monday\",\"tasks\": [\"Pick Up John\",\"Buy Milk\",\"Pay Bill\"]}|\n",
      "+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "todos = \"\"\"{\"day\": \"Monday\",\"tasks\": [\"Pick Up John\",\"Buy Milk\",\"Pay Bill\"]}\"\"\"\n",
    "\n",
    "todoStrDF = spark.createDataFrame([(todos,)], [\"todos_str\"])\n",
    "print(todoStrDF.printSchema())\n",
    "todoStrDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON approach requires a new data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for the JSON string\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"day\", T.StringType(), True),\n",
    "    T.StructField(\"tasks\", T.ArrayType(T.StringType()), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- todos_json: struct (nullable = true)\n",
      " |    |-- day: string (nullable = true)\n",
      " |    |-- tasks: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n",
      "+--------------------------------------------+\n",
      "|todos_json                                  |\n",
      "+--------------------------------------------+\n",
      "|{Monday, [Pick Up John, Buy Milk, Pay Bill]}|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the JSON string into a structured DataFrame\n",
    "parsedDF = todoStrDF.select(F.from_json(F.col(\"todos_str\"), schema=schema).alias(\"todos_json\"))\n",
    "\n",
    "print(parsedDF.printSchema())\n",
    "parsedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// retrieving value out of struct data type using the getItem function of\n",
    "Column class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   day|\n",
      "+------+\n",
      "|Monday|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsedDF.select(F.col(\"todos_json.day\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------+------------+\n",
      "|day   |tasks                             |first_task  |\n",
      "+------+----------------------------------+------------+\n",
      "|Monday|[Pick Up John, Buy Milk, Pay Bill]|Pick Up John|\n",
      "+------+----------------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parsedDF.select(\n",
    "    F.col(\"todos_json.day\"), \n",
    "    F.col(\"todos_json.tasks\"),\n",
    "    F.col(\"todos_json.tasks\")[0].alias(\"first_task\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|to_json(todos_json)                                            |\n",
      "+---------------------------------------------------------------+\n",
      "|{\"day\":\"Monday\",\"tasks\":[\"Pick Up John\",\"Buy Milk\",\"Pay Bill\"]}|\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parsedDF.select(F.to_json(\"todos_json\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worth Mention Functions\n",
    "This section covers the following functions: monotonically_\n",
    "increasing_id, when, coalesce, and lit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHEN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|dow|\n",
      "+---+---+\n",
      "|  1|Mon|\n",
      "|  2|Tue|\n",
      "|  3|Wed|\n",
      "|  4|Thu|\n",
      "|  5|Fri|\n",
      "|  6|Sat|\n",
      "|  7|Sun|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame with values from 1 to 7 to represent each day of the week\n",
    "dayOfWeekDF = spark.range(1,8,1).toDF(\"id\")\n",
    "# convert each numerical value to a string\n",
    "dayOfWeekDF.select(\"id\", \n",
    "                   F.when(F.col(\"id\") == 1, \"Mon\")\n",
    "                    .when(F.col(\"id\") == 2, \"Tue\")\n",
    "                    .when(F.col(\"id\") == 3, \"Wed\")\n",
    "                    .when(F.col(\"id\") == 4, \"Thu\")\n",
    "                    .when(F.col(\"id\") == 5, \"Fri\")\n",
    "                    .when(F.col(\"id\") == 6, \"Sat\")\n",
    "                    .when(F.col(\"id\") == 7, \"Sun\").alias(\"dow\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COALESCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a movie with null title\n",
    "schema_ = \"`actor_name` String, `movie_title` String, `produced_year` Long\"\n",
    "badMoviesDF = spark.createDataFrame( [\n",
    "    (None, None, 2018), (\"John Doe\", \"Awesome Movie\", 2018)], \n",
    "    schema = schema_)\n",
    "# use coalesce function to handle null value in the title column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|new_title|\n",
      "+---------+\n",
      "|  no_name|\n",
      "| John Doe|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "badMoviesDF.select(F.coalesce(\"actor_name\", F.lit(\"no_name\")).alias(\"new_title\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions (UDFs)\n",
    " UDFs must be\n",
    "registered with Spark before they are used, so Spark knows to ship them to executors to\n",
    "be used and executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the letterGrade function in Python\n",
    "def letterGrade(score):\n",
    "    if score > 100:\n",
    "        return \"Cheating\"\n",
    "    elif score >= 90:\n",
    "        return \"A\"\n",
    "    elif score >= 80:\n",
    "        return \"B\"\n",
    "    elif score >= 70:\n",
    "        return \"C\"\n",
    "    else:\n",
    "        return \"F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the function as a UDF\n",
    "letterGradeUDF = udf(letterGrade, T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|score|letter_grade|\n",
      "+-----+------------+\n",
      "|   95|           A|\n",
      "|   85|           B|\n",
      "|   75|           C|\n",
      "|   65|           F|\n",
      "|  110|    Cheating|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example DataFrame\n",
    "data = [(95,), (85,), (75,), (65,), (110,)]\n",
    "df = spark.createDataFrame(data, [\"score\"])\n",
    "\n",
    "# Use the UDF in a DataFrame\n",
    "df = df.withColumn(\"letter_grade\", letterGradeUDF(df[\"score\"]))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
