






from pyspark.sql import SparkSession
# Create a SparkSession
spark = (SparkSession
.builder
.appName("SparkSQLExampleApp")
.getOrCreate())


# Path to data set
csv_file = "./learning-spark-v2/flights/departuredelays.csv"





# In Python
schema = "`date` STRING, `delay` INT, `distance` INT,`origin` STRING, `destination` STRING"
# load data

df = (spark.read.format("csv")
.schema(schema) #.option("inferSchema", "true")
.option("header", "true")
.load(csv_file))





print(df.printSchema())
df.show(5)


df.createOrReplaceTempView("us_delay_flights_tbl")








spark.sql("""SELECT distance, origin, destination
FROM us_delay_flights_tbl WHERE distance > 1000
ORDER BY distance DESC""").show(10)





spark.sql("""SELECT date, delay, origin, destination
FROM us_delay_flights_tbl
WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'
ORDER by delay DESC""").show(10)








spark.sql("""SELECT date,
to_timestamp(date, 'MMddHHmm') AS timestamp,
date_format(to_timestamp(date, 'MMddHHmm'), 'MM-dd hh:mm a') AS formatted_timestamp
FROM us_delay_flights_tbl
WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'
ORDER by delay DESC
""").show(10)


spark.sql("""SELECT year(to_timestamp(date, 'MMddHHmm')) as YEAR, month(to_timestamp(date, 'MMddHHmm')) AS MONTH,
COUNT(delay) AS TotalDelayMinutes
FROM us_delay_flights_tbl
WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'
GROUP BY year(to_timestamp(date, 'MMddHHmm')), month(to_timestamp(date, 'MMddHHmm'))
""").show(10)





query = """
SELECT delay, origin, destination, to_timestamp(date, 'MMddHHmm') AS timestamp,
CASE
    WHEN delay >= 360 THEN 'Very Long Delays'
    WHEN delay >= 120 AND delay < 360 THEN 'Long Delay'
    WHEN delay >= 60 AND delay < 120 THEN 'Short Delay'
    WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'
    WHEN delay = 0 THEN 'No Delays'
    ELSE 'Early' 
END As Flight_Delay
FROM us_delay_flights_tbl
ORDER BY delay DESC
"""


spark.sql(query).show(20)


spark.stop()








# Python
spark.sql("CREATE DATABASE learn_spark_db")
spark.sql("USE learn_spark_db")





# CREATE TABLE 
spark.sql("CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)")


# INSERT INTO
spark.sql("INSERT INTO TABLE managed_us_delay_flights_tbl SELECT * FROM csv.`/databricks-datasets/learning-spark-v2/flights/departuredelays.csv`")






# Path to our US flight delays CSV file
csv_file = "/databricks-datasets/learning-spark-v2/flights/departuredelays.csv"
# Schema as defined in the preceding example
schema="date STRING, delay INT, distance INT, origin STRING, destination STRING"
flights_df = spark.read.csv(csv_file, schema=schema)
#flights_df.write.saveAsTable("managed_us_delay_flights_tbl")








spark.sql("""CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,
distance INT, origin STRING, destination STRING)
USING csv OPTIONS (PATH '/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')""")











# In Python
df_sfo = spark.sql("SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'")
df_jfk = spark.sql("SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'")





# Create a temporary and global temporary view
df_sfo.createOrReplaceGlobalTempView("us_origin_airport_SFO_global_tmp_view")
df_jfk.createOrReplaceTempView("us_origin_airport_JFK_tmp_view") # use _tmp_view to identify views





# read view like a table

# In Scala/Python
spark.read.table("us_origin_airport_JFK_tmp_view")
# Or
spark.sql("SELECT * FROM us_origin_airport_JFK_tmp_view")






# In Scala/Python
spark.catalog.dropGlobalTempView("us_origin_airport_SFO_global_tmp_view")
spark.catalog.dropTempView("us_origin_airport_JFK_tmp_view")








# In Scala/Python
spark.catalog.listDatabases()
spark.catalog.listTables()
spark.catalog.listColumns("us_delay_flights_tbl")





# SQL
query = '''CACHE [LAZY] TABLE <table-name>
UNCACHE TABLE <table-name>'''





# way 1
us_flights_df = spark.sql("SELECT * FROM us_delay_flights_tbl")
# way 2
us_flights_df2 = spark.table("us_delay_flights_tbl")














#few recommended usage patterns:
DataFrameWriter.format(args)
.option(args)
.bucketBy(args)
.partitionBy(args)
.save(path)

DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)











# In Python READ
file = """/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/
2010-summary.parquet/"""
df = spark.read.format("parquet").load(file)


# WRITE
# In Python
(df.write.format("parquet")
.mode("overwrite")
.option("compression", "snappy")
.save("/tmp/data/parquet/df_parquet"))





# In Python
from pyspark.sql.types import LongType

# Create cubed function
def cubed(s):
  return s * s * s

# Register UDF
spark.udf.register("cubed", cubed, LongType())

# Generate temporary view
spark.range(1, 9).createOrReplaceTempView("udf_test")

#Use it
spark.sql("SELECT id, cubed(id) AS id_cubed FROM udf_test").show()





spark.sql("SELECT s FROM test1 WHERE s IS NOT NULL AND strlen(s) > 1")
# Here the s is NOT NULL clause may not be executed prior to the strlen(s) > 1 clause



